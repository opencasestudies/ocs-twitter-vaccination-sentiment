knitr::opts_chunk$set(echo = TRUE)
100*(1 - sum(col == "grey")/length(col))
knitr::opts_chunk$set(echo = TRUE)
library(tidytext)
library(dplyr)
library(ggplot2)
library(wordcloud)
library(SnowballC)
library(igraph)
library(threejs)
library(visNetwork)
#tweets_df <- read.csv("data/since11_27_n500_hashtag_antivax4.csv",stringsAsFactors = FALSE)
tweets_df <- read.csv("data/date01_14_n18k_key2_uuidgens_final.csv",stringsAsFactors = FALSE)
head(tweets_df)
#tweets_df <- tweets_df[!duplicated(tweets_df$text),]
#head(tweets_df$text)
text_df <- tweets_df %>% select("text")
text_df$int <- c(1:length(text_df$text))
text_df <- text_df %>%
unnest_tokens(word, text)
head(text_df)
data(stop_words)
stop_words %>%
group_by(lexicon) %>%
tally()
text_df <- text_df %>%
anti_join(stop_words)
head(text_df,10)
text_df <- text_df %>%
mutate(word = wordStem(word))
head(text_df,10)
head(text_df %>% dplyr::count(word, sort = TRUE),10)
text_df %>%
count(word, sort = TRUE) %>%
filter(n>1700) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
ylab(NULL) +
coord_flip()
bigrams <- text_df %>%
unnest_tokens(bigram, word, token = "ngrams", n = 2)
head(bigrams %>%
count(bigram, sort = TRUE),10)
text_df %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
head(sentiments)
head(get_sentiments('afinn'))
nrc_example <- get_sentiments("afinn") %>%
filter(score == -2)
head(text_df %>%
inner_join(nrc_example) %>%
count(word, sort = TRUE),10)
library(tidyr)
tweet_sentiment <- text_df %>%
anti_join(stop_words) %>%
inner_join(get_sentiments("afinn")) %>%
group_by(int) %>%
mutate(text = paste0(word)) %>%
summarize(sentiment_mean = mean(score))
tweet_sentiment_ordered <- tweet_sentiment[order(tweet_sentiment$sentiment_mean),]
head(tweet_sentiment_ordered)
tweet_sentiment_ordered$int <- factor(tweet_sentiment_ordered$int, levels = tweet_sentiment_ordered$int)
ggplot(tweet_sentiment_ordered, aes(sentiment_mean)) +
geom_histogram(show.legend = FALSE, binwidth = .3) +
ggtitle("Mean afinn sentiment score per tweet") +
geom_vline(xintercept = 0, linetype = "dashed", colour = "red") +
ylab(NULL)
# #facet_wrap(~book, ncol = 2, scales = "free_x")
positive <- tweet_sentiment$int[tweet_sentiment$sentiment_mean > 2]
head(tweets_df$text[positive])
negative <- tweet_sentiment$int[tweet_sentiment$sentiment_mean < -2 ]
head(tweets_df$text[negative])
library(stringr)
pat <- c("[\r\n]|&amp.|@.*?\\S+|@.*?$|^RT |https?:.\\S+|https?:.*$")
tweets_df$text <-
tweets_df$text %>%
str_replace_all(pattern = pat, "")
tweets_df$retweet_text <-
tweets_df$retweet_text %>%
str_replace_all(pattern = pat, "")
head(tweets_df$text)
sum((tweets_df$location == "")==FALSE)/dim(tweets_df)[1]*100
t1 <- tweets_df$retweet_text[(is.na(tweets_df$retweet_text) == FALSE)]
t2 <- tweets_df$text[(is.na(tweets_df$retweet_text) == FALSE)]
sum((t1==t2) == FALSE)/length(t1)*100
tweets_df$rt_is_identical <-
(tweets_df$retweet_text == tweets_df$text)
head(tweets_df$rt_is_identical)
network_df <-
tweets_df %>%
filter(rt_is_identical != FALSE) %>%
select(text, uuidgens_user) %>%
group_by(text, uuidgens_user)
head(network_df)
network_df$n <- 0
new_df <- tweets_df %>%
filter(rt_is_identical != FALSE) %>%
select(uuidgens_user,uuidgens_retweet) %>%
na.omit() %>%
group_by(uuidgens_user, uuidgens_retweet) %>%
tally()
colnames(new_df) <- colnames(network_df)
network_df <- rbind(network_df, new_df)
seeds_df <- read.csv("data/seeds_WHO_HR_friends.csv",stringsAsFactors = FALSE)
head(seeds_df)
seed_users <- seeds_df[(dim(seeds_df)[1] - 1) : dim(seeds_df)[1],]
friends_df <- seeds_df[1:(dim(seeds_df)[1] - 2),]
vtx_df <- data.frame("unique_vtcs" = unique(c(network_df$text, network_df$uuidgens_user)))
vtx_df <- merge(vtx_df, seeds_df, by.x = "unique_vtcs", by.y = "uuidgens_user", all = TRUE)
vtx_df$pro_vax[vtx_df$pro_vax == 0] <- 2
g <- graph_from_data_frame(d = network_df, directed = FALSE, vertices = vtx_df)
randn <- c(3:(sum(is.na(vtx_df$pro_vax) == TRUE)+2))
vtx_df$pro_vax[is.na(vtx_df$pro_vax) == TRUE] <- randn
lpc <- cluster_label_prop(g, initial = vtx_df$pro_vax)
clp <- cluster_label_prop(g, initial = vtx_df$pro_vax)
# create some vertex attributes
V(g)$community <- clp$membership
V(g)$betweenness <- betweenness(g, v = V(g), directed = F)
V(g)$degree <- degree(g, v = V(g))
data <- toVisNetworkData(g)
nodes <- data[[1]]
edges <- data[[2]]
library(RColorBrewer)
get_big_coms <- nodes %>%
select(community) %>%
group_by(community) %>%
tally() %>%
arrange(desc(n))
biggest_groups<- get_big_coms$community[1:2]
col <- rep("grey", length(V(g)))
colors<- brewer.pal(10, "Paired")
for(j in c(1:2)){
col[V(g)$community == biggest_groups[j]] <- colors[j]
}
#graphjs(g, vertex.color = col,  vertex.size = .3, na.ok = TRUE)
vtxattr_df <- data.frame(vertex_attr(g))
#head(vtxattr_df)
pro_communities<- vtxattr_df %>% filter(pro_vax == 1) %>% select(community) %>% group_by(community)# %>% tally()
anti_communities<- vtxattr_df %>% filter(pro_vax == 2) %>% select(community) %>% group_by(community)# %>% tally()
vtxattr_df %>% filter(community == 47) %>% tally()
vtxattr_df %>% filter(community == 46) %>% tally()
library(RColorBrewer)
col <- rep("grey", length(V(g)))
colors<- brewer.pal(10, "Paired")
for(i in pro_communities){
col[V(g)$community == i] <- colors[1]
}
for(i in anti_communities){
col[V(g)$community == i] <- colors[2]
}
graphjs(g, vertex.color = col,  vertex.size = .3, na.ok = TRUE)
100*(1 - sum(col == "grey")/length(col))
100*sum(col == colors[1])/length(col)
100*sum(col == colors[2])/length(col)
colors
vtxattr_df$colors <- col
new_provax <- vtxattr_df %>% filter(is.na(pro_vax)) %>% filter(colors == col[2]) %>% select(name)
new_provax[sample(nrow(new_provax), 10), ]
vtxattr_df <- data.frame(vertex_attr(g))
#head(vtxattr_df)
vtxattr_df %>% filter(pro_vax == 1) %>% select(community) %>% group_by(community)# %>% tally()
vtxattr_df %>% filter(pro_vax == 2) %>% select(community) %>% group_by(community)# %>% tally()
pro_communities<- vtxattr_df %>% filter(pro_vax == 1) %>% select(community) %>% group_by(community)# %>% tally()
anti_communities<- vtxattr_df %>% filter(pro_vax == 2) %>% select(community) %>% group_by(community)# %>% tally()
vtxattr_df %>% filter(community == 47) %>% tally()
vtxattr_df %>% filter(community == 46) %>% tally()
vtxattr_df <- data.frame(vertex_attr(g))
#head(vtxattr_df)
vtxattr_df %>% filter(pro_vax == 1) %>% select(community) %>% group_by(community) %>% tally()
vtxattr_df %>% filter(pro_vax == 2) %>% select(community) %>% group_by(community) %>% tally()
pro_communities<- vtxattr_df %>% filter(pro_vax == 1) %>% select(community) %>% group_by(community)# %>% tally()
anti_communities<- vtxattr_df %>% filter(pro_vax == 2) %>% select(community) %>% group_by(community)# %>% tally()
vtxattr_df %>% filter(community == 47) %>% tally()
vtxattr_df %>% filter(community == 46) %>% tally()
labeled <- 100*(1 - sum(col == "grey")/length(col))
pro_labeled <- 100*sum(col == colors[1])/length(col)
anti_labeled <- 100*sum(col == colors[2])/length(col)
colors
labeled <- 100*(1 - sum(col == "grey")/length(col))
pro_labeled <- 100*sum(col == colors[1])/length(col)
anti_labeled <- 100*sum(col == colors[2])/length(col)
head(tweets_df$text)
library(stringr)
pat <- c("[\r\n]|&amp.|@.*?\\S+|@.*?$|^RT |https?:.\\S+|https?:.*$")
tweets_df$text <-
tweets_df$text %>%
str_replace_all(pattern = pat, "")
tweets_df$retweet_text <-
tweets_df$retweet_text %>%
str_replace_all(pattern = pat, "")
head(tweets_df$text)
library(stringr)
pat <- "[\r\n]|&amp.|@.*?\\S+|@.*?$|^RT |https?:.\\S+|https?:.*$"
tweets_df$text <-
tweets_df$text %>%
str_replace_all(pattern = pat, "")
tweets_df$retweet_text <-
tweets_df$retweet_text %>%
str_replace_all(pattern = pat, "")
head(tweets_df$text)
library(stringr)
library(kableExtra)
pat <- "[\r\n]|&amp.|@.*?\\S+|@.*?$|^RT |https?:.\\S+|https?:.*$"
tweets_df$text <-
tweets_df$text %>%
str_replace_all(pattern = pat, "")
tweets_df$retweet_text <-
tweets_df$retweet_text %>%
str_replace_all(pattern = pat, "")
kable(head(tweets_df$text))
library(kableExtra)
kable(head(tweets_df$text))
library(kableExtra)
library(tidytext)
library(dplyr)
library(ggplot2)
library(wordcloud)
library(SnowballC)
library(igraph)
library(threejs)
library(visNetwork)
#tweets_df <- read.csv("data/since11_27_n500_hashtag_antivax4.csv",stringsAsFactors = FALSE)
tweets_df <- read.csv("data/date01_14_n18k_key2_uuidgens_final.csv",stringsAsFactors = FALSE)
kable(head(tweets_df))
library(kableExtra)
library(tidytext)
library(dplyr)
library(ggplot2)
library(wordcloud)
library(SnowballC)
library(igraph)
library(threejs)
library(visNetwork)
#tweets_df <- read.csv("data/since11_27_n500_hashtag_antivax4.csv",stringsAsFactors = FALSE)
tweets_df <- read.csv("data/date01_14_n18k_key2_uuidgens_final.csv",stringsAsFactors = FALSE)
#kable(head(tweets_df))
head(tweets_df$text)
library(stringr)
pat <- "[\r\n]|&amp.|@.*?\\S+|@.*?$|https?:.\\S+|https?:.*$"
tweets_df$text <-
tweets_df$text %>%
str_replace_all(pattern = pat, "")
tweets_df$retweet_text <-
tweets_df$retweet_text %>%
str_replace_all(pattern = pat, "")
head(tweets_df$text)
t1 <- tweets_df$retweet_text[(is.na(tweets_df$retweet_text) == FALSE)]
t2 <- tweets_df$text[(is.na(tweets_df$retweet_text) == FALSE)]
sum((t1==t2) == FALSE)/length(t1)*100
is_retweet <- (is.na(tweets_df$retweet_text) == FALSE)
t1 <- tweets_df$retweet_text[is_retweet]
t2 <- tweets_df$text[is_retweet]
sum((t1==t2) == FALSE)/length(t1)*100
is_retweet <- is.na(tweets_df$retweet_text) == FALSE
t1 <- tweets_df$retweet_text[is_retweet]
t2 <- tweets_df$text[is_retweet]
sum((t1==t2) == FALSE)/length(t1)*100
pat <- "[\r\n]|&amp.|@.*?\\S+|@.*?$|https?:.*\\S+|https?:.*$"
tweets_df$text <-
tweets_df$text %>%
str_replace_all(pattern = pat, "")
tweets_df$retweet_text <-
tweets_df$retweet_text %>%
str_replace_all(pattern = pat, "")
head(tweets_df$text)
is_retweet <- is.na(tweets_df$retweet_text) == FALSE
t1 <- tweets_df$retweet_text[is_retweet]
t2 <- tweets_df$text[is_retweet]
sum((t1==t2) == FALSE)/length(t1)*100
tweets_df <- read.csv("data/date01_14_n18k_key2_uuidgens_final.csv",stringsAsFactors = FALSE)
is_retweet <- is.na(tweets_df$retweet_text) == FALSE
t1 <- tweets_df$retweet_text[is_retweet]
t2 <- tweets_df$text[is_retweet]
sum((t1==t2) == FALSE)/length(t1)*100
tweets_df$rt_is_identical <-
(tweets_df$retweet_text == tweets_df$text)
head(tweets_df$rt_is_identical)
network_df <-
tweets_df %>%
filter(rt_is_identical != FALSE) %>%
select(text, uuidgens_user, uuidgens_retweet) %>%
group_by(text) %>%
tally(uuidgens_user, uuidgens_retweet)
network_df <-
tweets_df %>%
filter(rt_is_identical != FALSE) %>%
select(text, uuidgens_user, uuidgens_retweet) %>%
group_by(text)
#network_df$n <- 0
# new_df <- tweets_df %>%
#   filter(rt_is_identical != FALSE) %>%
#   select(uuidgens_user,uuidgens_retweet) %>%
#   na.omit() %>%
#   group_by(uuidgens_user, uuidgens_retweet) %>%
#   tally()
#
#
# colnames(new_df) <- colnames(network_df)
# network_df <- rbind(network_df, new_df)
head(network_df)
network_df <-
tweets_df %>%
filter(rt_is_identical != FALSE) %>%
select(text, uuidgens_user, uuidgens_retweet) %>%
group_by(text) %>%
tally()
#network_df$n <- 0
# new_df <- tweets_df %>%
#   filter(rt_is_identical != FALSE) %>%
#   select(uuidgens_user,uuidgens_retweet) %>%
#   na.omit() %>%
#   group_by(uuidgens_user, uuidgens_retweet) %>%
#   tally()
#
#
# colnames(new_df) <- colnames(network_df)
# network_df <- rbind(network_df, new_df)
head(network_df)
network_df <-
tweets_df %>%
filter(rt_is_identical != FALSE) %>%
select(text, uuidgens_user, uuidgens_retweet) %>%
group_by(text) %>%
unique(uuidgens_user, uuidgens_retweet)
network_df <-
tweets_df %>%
filter(rt_is_identical != FALSE) %>%
select(text, uuidgens_user, uuidgens_retweet) %>%
group_by(text) %>%
#unique(uuidgens_user, uuidgens_retweet)
summarise(Unique_Elements = unique(uuidgens_user))
network_df <-
tweets_df %>%
filter(rt_is_identical != FALSE) %>%
select(text, uuidgens_user, uuidgens_retweet) %>%
group_by(text) %>%
#unique(uuidgens_user, uuidgens_retweet)
summarise(Unique_Elements = unique(uuidgens_user))
# network_df <-
#   tweets_df %>%
#   filter(rt_is_identical != FALSE) %>%
#   select(text, uuidgens_user, uuidgens_retweet) %>%
#   group_by(text) %>%
#   #unique(uuidgens_user, uuidgens_retweet)
#   summarise(Unique_Elements = unique(uuidgens_user))
distinct(tweets_df, text, uuidgens_user)
#network_df$n <- 0
# new_df <- tweets_df %>%
#   filter(rt_is_identical != FALSE) %>%
#   select(uuidgens_user,uuidgens_retweet) %>%
#   na.omit() %>%
#   group_by(uuidgens_user, uuidgens_retweet) %>%
#   tally()
#
#
# colnames(new_df) <- colnames(network_df)
# network_df <- rbind(network_df, new_df)
#head(network_df)
# network_df <-
#   tweets_df %>%
#   filter(rt_is_identical != FALSE) %>%
#   select(text, uuidgens_user, uuidgens_retweet) %>%
#   group_by(text) %>%
#   #unique(uuidgens_user, uuidgens_retweet)
#   summarise(Unique_Elements = unique(uuidgens_user))
distinct(tweets_df, text, uuidgens_user, uuidgens_retweet)
#network_df$n <- 0
# new_df <- tweets_df %>%
#   filter(rt_is_identical != FALSE) %>%
#   select(uuidgens_user,uuidgens_retweet) %>%
#   na.omit() %>%
#   group_by(uuidgens_user, uuidgens_retweet) %>%
#   tally()
#
#
# colnames(new_df) <- colnames(network_df)
# network_df <- rbind(network_df, new_df)
#head(network_df)
# network_df <-
#   tweets_df %>%
#   filter(rt_is_identical != FALSE) %>%
#   select(text, uuidgens_user, uuidgens_retweet) %>%
#   group_by(text) %>%
#   #unique(uuidgens_user, uuidgens_retweet)
#   summarise(Unique_Elements = unique(uuidgens_user))
distinct(tweets_df, text)
#network_df$n <- 0
# new_df <- tweets_df %>%
#   filter(rt_is_identical != FALSE) %>%
#   select(uuidgens_user,uuidgens_retweet) %>%
#   na.omit() %>%
#   group_by(uuidgens_user, uuidgens_retweet) %>%
#   tally()
#
#
# colnames(new_df) <- colnames(network_df)
# network_df <- rbind(network_df, new_df)
#head(network_df)
# network_df <-
#   tweets_df %>%
#   filter(rt_is_identical != FALSE) %>%
#   select(text, uuidgens_user, uuidgens_retweet) %>%
#   group_by(text) %>%
#   #unique(uuidgens_user, uuidgens_retweet)
#   summarise(Unique_Elements = unique(uuidgens_user))
unique(tweets_df, text)
# network_df <-
#   tweets_df %>%
#   filter(rt_is_identical != FALSE) %>%
#   select(text, uuidgens_user, uuidgens_retweet) %>%
#   group_by(text) %>%
#   #unique(uuidgens_user, uuidgens_retweet)
#   summarise(Unique_Elements = unique(uuidgens_user))
unique(tweets_df$text)
#network_df$n <- 0
# new_df <- tweets_df %>%
#   filter(rt_is_identical != FALSE) %>%
#   select(uuidgens_user,uuidgens_retweet) %>%
#   na.omit() %>%
#   group_by(uuidgens_user, uuidgens_retweet) %>%
#   tally()
#
#
# colnames(new_df) <- colnames(network_df)
# network_df <- rbind(network_df, new_df)
#head(network_df)
# network_df <-
#   tweets_df %>%
#   filter(rt_is_identical != FALSE) %>%
#   select(text, uuidgens_user, uuidgens_retweet) %>%
#   group_by(text) %>%
#   #unique(uuidgens_user, uuidgens_retweet)
#   summarise(Unique_Elements = unique(uuidgens_user))
distinct(tweets_df, text)
#network_df$n <- 0
# new_df <- tweets_df %>%
#   filter(rt_is_identical != FALSE) %>%
#   select(uuidgens_user,uuidgens_retweet) %>%
#   na.omit() %>%
#   group_by(uuidgens_user, uuidgens_retweet) %>%
#   tally()
#
#
# colnames(new_df) <- colnames(network_df)
# network_df <- rbind(network_df, new_df)
#head(network_df)
