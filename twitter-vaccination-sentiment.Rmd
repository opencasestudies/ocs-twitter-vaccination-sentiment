---
title: Twitter Vaccination Sentiment Analysis
author: "Alexandra Stephens"
output: 
    html_document:
        theme: cosmo 
        toc: true
        toc_float: true
        highlight: tango
        number_sections: false
---

```{r setup, include=FALSE, warnings=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Motivation

Vaccination has become a disputed topic in recent years. Parents are making decisions whether or not to vaccinate their kids, in which the latter case puts them at higher risk of contracting diseases. Therefore it is important to identify communities who are at higher risk of infectious diseases in order to implement preventative measures. In this document, we will attempt to classify public opinion by analyzing sentiment towards vaccination on Twitter.

This code primarily consists of data exploration and visualization using Natural Language Processing techniques along with a machine learning approach to classifying sides of a debate.

The libraries used in this study are `tidytext`, `dplyr`, `ggplot2`, `wordcloud`, `SnowballC` ... In order to run this code please ensure you have these packages installed.

The learning objectives include data cleaning and manipulation with Natural Language Processing, label propagation, Naive Bayes classification, other ML classification algorithms.

# What is the data?

In a separate R file, a Twitter API call was used to download the data. Instructions to implement this can be found [here](https://medium.com/@GalarnykMichael/accessing-data-from-twitter-api-using-r-part1-b387a1c7d3e). The keywords and hashtags searched were "#antivax", "#vaccineswork", and "vaccination."

The API call included `n = 10000` thus we have 10,000 tweets downloaded. In the separate file (which will later be included in this repository), all identifiers have been removed including Twitter handles within a Tweet, which appear when a user is replying to or re-tweeting a Tweet.

# Data import
First we import the libraries needed, then upload the data with `read.csv`.
Make sure you have the libraries below installed before running the code.
```{r, message = FALSE}
library(tidytext) 
library(dplyr)
library(ggplot2)
library(wordcloud)
library(SnowballC)

library(igraph)
library(threejs)
library(visNetwork)

#tweets_df <- read.csv("data/since11_27_n500_hashtag_antivax4.csv",stringsAsFactors = FALSE)
tweets_df <- read.csv("data/date01_14_n18k_key2_uuidgens_final.csv",stringsAsFactors = FALSE)
head(tweets_df)
```


# Data wrangling
Since the Twitter API captures all tweets, including those that are retweets, we should remove duplicate Tweets. In this analysis we simply want to categorize the Tweets, so we won't need to keep track of which tweets are retweets or how many times a Tweet is retweeted or favorited (i.e. how popular it is). In any case, there is still the `retweetCount` column that keeps track of this if needed.

```{r}
#tweets_df <- tweets_df[!duplicated(tweets_df$text),]
#head(tweets_df$text)
```
##The tidy text and NLP
The tidy text format is using **one-token-per-row** in our dataframe. In short, tokens in natural language processing are words. To be technical, you could say a token is a list of characters between two spaces. Tokenization is the process of breaking up a string, or paragraph or document into word bits.

So we want to tokenize our Tweets, however we don't want to lose any information. Specifically, we want to keep track of what Tweet each word came from since our analysis involves differentiating Tweets. Other analyses including topic summarization may not require keeping track of this--you can just a create a jumble (er, "corpus") of words and find commonalities.

Thus we add the `int` column to the original dataframe `text_df` as an index for the Tweets. We then tokenize the text data with tidy text `unnest_tokens` and create a new dataframe `text_df`.

```{r}
text_df <- tweets_df %>% select("text")
text_df$int <- c(1:length(text_df$text))
text_df <- text_df %>%
    unnest_tokens(word, text)
head(text_df)
```

Next we want to remove stop words from the text. Stop words in NLP are common words that don't add value to a sentence, paragraph, document etc. For example, "still think the flu shot is" doesn't give us any extra information than "still think flu shot," so we can remove these words to make our dataset more concise.

We load the `stop_words` dataset, which has two columns: `word` and `lexicon`. There are three different lexicons that reference the *source* of the stop word: ["onix"](http://www.lextek.com/manuals/onix/stopwords1.html), ["SMART"](http://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf), and ["snowball."](http://snowball.tartarus.org/algorithms/english/stop.txt) We count the number of stop words in each category so that we can modify this if needed. (For example, if by removing all possible stop words we aren't left with enough information, we could instead just remove the 174 `snowball` stopwords).

```{r}
data(stop_words)

stop_words %>% 
    group_by(lexicon) %>% 
    tally()

text_df <- text_df %>%
  anti_join(stop_words)

head(text_df,10)
```
##Word stemming
Another technique often used alongside removing stopwords is token *stemming*. Stemming changes each word to a root word so that different conjugations of the same word are not treated as different words completely. A human can tell that "vaccinate" and "vaccination" have similar meaning.On the computer, if we were to count every repetition of "vaccinate" we would not get an accurate total since "vaccination" would be ignored.

We will use the library `SnowballC` which implements the [Porter stemming algorithm](http://snowball.tartarus.org/algorithms/porter/stemmer.html).

```{r}
text_df <- text_df %>%
  mutate(word = wordStem(word))
head(text_df,10)
```

# Exploratory data analysis

A common NLP data exploration practice is looking at unigram frequencies.
*N-grams* are sequences of N items/tokens. Common n-grams that are used in NLP are unigrams, bigrams, and trigrams. We'll dive deeper into applications of n-grams later (note to self), but we introduce the topic now because it is common to find n-gram frequencies, as we do below with unigrams.
```{r}
head(text_df %>% dplyr::count(word, sort = TRUE),10)
```
We can graph the unigram frequencies with ggplot for a nice visualization of our text data.
```{r}
text_df %>%
  count(word, sort = TRUE) %>%
  filter(n>1700) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  ylab(NULL) +
  coord_flip()
```
Similarly, we can look at common *bigrams*, which are two words in a row.
```{r}
bigrams <- text_df %>%
  unnest_tokens(bigram, word, token = "ngrams", n = 2)

head(bigrams %>%
  count(bigram, sort = TRUE),10)
```


For another fun exploration graphic, try word clouds!
```{r}
text_df %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

##Sentiment
We want to classify the Tweets by sentiment, or rather by the feelings and emotions portrayed by the user. As humans we can easily tell if someone is angry, sad, satisfied, or even sarcastic. However we have to give the computer some clues on how to do this start with the `sentiments` dataset which is loaded with the `tidytext` library. Similar to the `stop_words` dataset, there are three lexicons:

* `afinn` gives each word an integer score between -5 and 5, from most negative to most positive sentiment.

* `bing` gives a binary "negative" or "positive" sentiment to each words.

* `nrc` assigns sentiment labels of "positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust."

We can also use the tidytext's `get_sentiments()` function to only get data from a particular lexicon.

```{r}
head(sentiments)
head(get_sentiments('afinn'))
```
"How were these sentiment lexicons put together and validated? They were constructed via either crowdsourcing (using, for example, Amazon Mechanical Turk) or by the labor of one of the authors, and were validated using some combination of crowdsourcing again, restaurant or movie reviews, or Twitter data. Given this information, we may hesitate to apply these sentiment lexicons to styles of text dramatically different from what they were validated on, such as narrative fiction from 200 years ago. While it is true that using these sentiment lexicons with, for example, Jane Austenâ€™s novels may give us less accurate results than with tweets sent by a contemporary writer, we still can measure the sentiment content for words that are shared across the lexicon and the text."
(note to self)

Next let's merge some of our data with the sentiment lexicon "afinn" to explore our text.
```{r}
nrc_example <- get_sentiments("afinn") %>% 
  filter(score == -2)

head(text_df %>%
  inner_join(nrc_example) %>%
  count(word, sort = TRUE),10)
```
We note that sentiment analysis would not be straight-forward. From the text printed above, we can't exactly pinpoint if the user is being negative about vaccinations, or the effects of not getting vaccinated, or just being negative about the anti-vaccination or vaccination communities.

Let's plot the average `afinn` scores for each tweet to explore the overall sentiment of our data.

```{r,message=FALSE}
library(tidyr)

tweet_sentiment <- text_df %>%
  anti_join(stop_words) %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(int) %>%
  mutate(text = paste0(word)) %>%
  summarize(sentiment_mean = mean(score))

tweet_sentiment_ordered <- tweet_sentiment[order(tweet_sentiment$sentiment_mean),] 
head(tweet_sentiment_ordered)

tweet_sentiment_ordered$int <- factor(tweet_sentiment_ordered$int, levels = tweet_sentiment_ordered$int)

ggplot(tweet_sentiment_ordered, aes(int, sentiment_mean)) +
  geom_col(show.legend = FALSE) +
  ggtitle("Mean afinn sentiment score per tweet") + 
  ylab(NULL)
 # #facet_wrap(~book, ncol = 2, scales = "free_x")


```

Let's print out some positive Tweets. We can use the index column "int" that we've kept constant to print out the original (sans and Twitter handles) Tweets.

```{r}
positive <- tweet_sentiment$int[tweet_sentiment$sentiment_mean > 2]
head(tweets_df$text[positive])
```
And let's look at some negative ones as well..
```{r}
negative <- tweet_sentiment$int[tweet_sentiment$sentiment_mean < -2 ]
head(tweets_df$text[negative])
```
Neither the positive nore negative Tweets are unanamously on either side of this "issue, even with this small sample of text. Now we can really see how this won't be easy!

We should think about what we can do about the words that won't be found in the `sentiments` dataset, like "#vaccineswork" that might give us even more information about whether or not the tweet is supporting vaccines or not.

Sentiment may be just a small part of clustering Tweets as "pro-vaccination" or "anti-vaccination."

http://www.public.asu.edu/~huanliu/papers/sbp14.pdf

# Data analysis
Throughout our data exploration, we realized the problem we are solving is not quite what we started with. This is not a sentiment analysis: this is classifying Tweets as one of two sides of a debate. 

Assuming that, with high probability, users do not change their views--expecially within a 7-day window, we can label Twitter users and represent them as a *social network* to help classify Tweets.

To do this, we will use the power of Retweets and what are called seed users, or prominent figures that clearly have one opion or another. For example, the World Health Organization and CDC are two entities that would put out pro-vaccination Tweets. The data we have also contains reply Tweets, but we cannot count on these to identify similar opinion because users may reply to Tweets they agree with or Tweets they do not agree with. We use Retweets because we can assume that if a user retweets another users Tweets about vaccination, they probably agree with their opinion. This assumption gets stronger as the social network grows more connections.

## Label propagation

```{r}
library(stringr)

pat <- c("[\r\n]|&amp.|@.*? |@.*?$|^RT |https:.* |https:.*$")

tweets_df$text <- 
  tweets_df$text %>%
    str_replace_all(pattern = pat, "")

tweets_df$retweet_text <- 
  tweets_df$retweet_text %>%
    str_replace_all(pattern = pat, "")

head(tweets_df$text)
```

Let's find the percentage of tweets in which the user has a location on their profile 
```{r}
sum((tweets_df$location == "")==FALSE)/dim(tweets_df)[1]*100
```
Next we find the proportion of retweets that are exactly the same as the text they retweeted. This is because we don't want to create an edge between two users when one of them retweeted something, and then added text to say they disagree with the tweet they retweeted.

```{r}
t1 <- tweets_df$retweet_text[(is.na(tweets_df$retweet_text) == FALSE)]
t2 <- tweets_df$text[(is.na(tweets_df$retweet_text) == FALSE)]

sum((t1==t2) == FALSE)/length(t1)*100
```
Only a small percentage of retweets added text. We can safely leave out this data and still have a large dataset.
Let's add a column that labels the retweets as identical or not (or na, if the tweet is not a retweet).
This leaves NAs where `tweets_df$text` is NA.
```{r}
tweets_df$rt_is_identical <- 
  (tweets_df$retweet_text == tweets_df$text)

head(tweets_df$rt_is_identical)
```
We need to build our network. To do this we will use the library `igraph`. First we should create a data frame where the first two columns are vertices with an edge between them. We will use both users and tweets as vertices, then add attributes if there are multiple connections necessary.

or we can do this: 
```{r}
network_df <- 
  tweets_df %>%
  filter(rt_is_identical != FALSE) %>%
  select(text, uuidgens_user) %>%
  group_by(text, uuidgens_user)
head(network_df)
```

So now we've connect each user to all of its tweets, and each unique tweet to all unique users that retweeted it.

Add user to user edges
```{r}
network_df$n <- 0

new_df <- tweets_df %>%
  filter(rt_is_identical != FALSE) %>%
  select(uuidgens_user,uuidgens_retweet) %>%
  na.omit() %>%
  group_by(uuidgens_user, uuidgens_retweet) %>%
  tally()

colnames(new_df) <- colnames(network_df)
network_df <- rbind(network_df, new_df)
```
IMPORT FRIENDS
The column "pro_vax" is a 1 for users who the world health organization follows, and a 0 for users HealthRanger, and anti-vax twitter account, follows. The package rtweet refers to users that a user follows as "friends".
```{r, warning=FALSE}
seeds_df <- read.csv("data/seeds_WHO_HR_friends.csv",stringsAsFactors = FALSE)
head(seeds_df)
seed_users <- seeds_df[(dim(seeds_df)[1] - 1) : dim(seeds_df)[1],]
friends_df <- seeds_df[1:(dim(seeds_df)[1] - 2),]
```
Create `vtx_df` so we can keep track of the order of vertices. Then we can add vertex attributes and the seed users.
```{r, message=FALSE, warning= FALSE}

vtx_df <- data.frame("unique_vtcs" = unique(c(network_df$text, network_df$uuidgens_user)))
vtx_df <- merge(vtx_df, seeds_df, by.x = "unique_vtcs", by.y = "uuidgens_user", all = TRUE)

vtx_df$pro_vax[vtx_df$pro_vax == 0] <- 2

g <- graph_from_data_frame(d = network_df, directed = FALSE, vertices = vtx_df)
head(V(g))

randn <- c(3:(sum(is.na(vtx_df$pro_vax) == TRUE)+2))
vtx_df$pro_vax[is.na(vtx_df$pro_vax) == TRUE] <- randn
lpc <- cluster_label_prop(g, initial = vtx_df$pro_vax)
```
Get a random sample and label anti-vax and pro-vax seed users.
```{r}
network_df_sample <- network_df[sample(nrow(network_df), 2000), ]

vtx_df <- data.frame("unique_vtcs" = unique(c(network_df_sample$text, network_df_sample$uuidgens_user)))
vtx_df <- merge(vtx_df, seeds_df, by.x = "unique_vtcs", by.y = "uuidgens_user", all = TRUE)

vtx_df$pro_vax[vtx_df$pro_vax == 0] <- 2

g <- graph_from_data_frame(d = network_df_sample, directed = FALSE, vertices = vtx_df)

# Add random labels for remaining vertices (initialized vertices)
randn <- c(3:(sum(is.na(vtx_df$pro_vax) == TRUE)+2))
vtx_df$pro_vax[is.na(vtx_df$pro_vax) == TRUE] <- randn

```

Run label propagation, Add vertex attriutes and color the largest communities.
```{r}
clp <- cluster_label_prop(g, initial = vtx_df$pro_vax)

# create some vertex attributes
V(g)$community <- clp$membership
V(g)$betweenness <- betweenness(g, v = V(g), directed = F)
V(g)$degree <- degree(g, v = V(g))


data <- toVisNetworkData(g)
nodes <- data[[1]]
edges <- data[[2]]

library(RColorBrewer)

get_big_coms <- nodes %>% 
  select(community) %>% 
  group_by(community) %>% 
  tally() %>%
  arrange(desc(n))

biggest_groups<- get_big_coms$community[1:10]

col <- rep("grey", length(V(g)))
colors<- brewer.pal(10, "Paired")

for(j in c(1:10)){
  col[V(g)$community == biggest_groups[j]] <- colors[j]
}
graphjs(g, vertex.color = col,  vertex.size = .3, na.ok = TRUE)

```


# Summary of results


