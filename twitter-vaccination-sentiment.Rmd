---
title: "Twitter Vaccination Sentiment Analysis"
author: "Alexandra Stephens"
output:
  html_document:
    highlight: tango
    number_sections: no
    theme: cosmo
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE, warnings=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Motivation
Vaccines have faced criticism from the beginning of development. However, recently in the United States and around the world, vaccination has faced increasing skepticism and vaccination rates have begun to decline. Anti-vaccination rhetoric is especially prevalent on social media where platforms serve as an open space for misinformation to spread.

The goal of this analysis is to download Twitter data and identify tweets as "pro" or "anti" vaccination. Tweets often have an associated location, thus this would allow researchers to locate communities where anti-vaccination sentiment is growing. This could help healthcare professionals identify communities that are at higher risk of infectious diseases.

In this document, we will provide start-to-finish methods on how to identify Twitter users and tweets that are "anti-vaccination."

<center>
![](data/map1.png)
</center>
This case study consists of data exploration and visualization using Natural Language Processing techniques, document clustering, and sentiment analysis.

The libraries used in this study are listed in the following table, along with their purpose in this particular case study:

|Library|Purpose|
|---|---|
|`stringr`|Parsing text with regular expressions|
|`tidytext`|Loading relevant NLP datasets and manipulating text data|
|`dplyr`|Dataframe manipulation|
|`ggplot2`|Plotting sentiment|
|`wordcloud`|Creating wordcloud visuals|
|`SnowballC`|Word stemming|
|`tm`|Document Term Matrix Class|
|`topicmodels`|Applying Latent Dirichlet allocation|
|`kableExtra`|Formatting output into tables|
|`localgeo`|Converting city data to latitude and longitude|

In order to run this code please ensure you have these packages installed. 

If you would like to download your own Twitter data, you will need to sign up for a Twitter developer account, create an application, and enter your own keys & credentials in the "What is the data?" section below.

The learning objectives for this case study include data cleaning and manipulation with natural language processing and regular expressions, document clustering/topic modeling, and sentiment analysis.

# What is the data?
To collect data from Twitter, we suggest using the `rtweet` package. If you would like to download your own Twitter data, you will need to sign up for a [Twitter developer account](https://developer.twitter.com/en/apply-for-access.html), create an application, and enter your own keys in the R code chunk below. Note that we've set the code chunk `eval = false` so it is ignored when this file is knit.

```{r eval=FALSE}
library(rtweet)

twitter_token <- create_token(
  app = "twitter_app_name",
  consumer_key = "XXXXXXXXXXXXXXX",
  consumer_secret = "XXXXXXXXXXXXXXXXXXXXX",
  access_token = "XXXXXXXXXXXXXXXXXXXXXXXXXXX",
  access_secret = "XXXXXXXXXXXXXXXXX")
```
Once the `twitter_token` is created it is stored in the workspace and you are free to use all the functions in `rtweet`!

The `search_tweets` function can download 18,000 every 15 minutes (with the free developer account). However adding `retryonratelimit = TRUE` automatically waits for the rate limit to be reset (after 15 min) and then searches for the remaining tweets. This is repeated until all requested tweets are downloaded.

We search for the 5,000 most recent tweets using the option `type = "recent"` that contain the terms "antivax", "provax", "vaccineswork", "vaccinesdontwork", or "vaccines". `include_rts = TRUE` means we are including retweets. This is important for this case study: we want to be able to connect users to one another if they share the same opinion. We will assume if a user retweets a tweet without adding any text to it (which might state their opposition to the tweet) then they agree with the tweet.

We set `lang = "en"` to capture only tweets in English.

```{r eval=FALSE}
st <- search_tweets('antivax OR vaccines OR vaccineswork OR vaccine OR vaccinesdontwork OR provax',
                    n = 5000,
                    type = "recent",
                    include_rts = TRUE, 
                    lang = "en")

st <- lat_lng(st) #note
```
After collecting the tweets with the `rtweet` package, we exported the data as a csv file. This is an important step because the data you collect with the API can be very different every time. It also takes a few minutes to download new tweets.

Before exporting as a CSV we removed the columns that contained lists because this is not compatible for export.
```{r eval=FALSE}
for (col in colnames(st))
  {
  if (typeof(st[[col]]) == "list")
    {
    st[[col]] <- NULL
  }
}
```

We've stored one such twitter dataset in the file __. This dataset will be used for the rest of the analysis. You can use this dataset or create your own with the above steps.

# Data import
Before doing anything, we should load the libraries needed for the case study.
```{r, message = FALSE}
library(dplyr)
library(tidytext)
library(ggplot2)
library(stringr)
library(kableExtra)
library(wordcloud)
library(SnowballC)
library(localgeo)
library(tm)
library(topicmodels)
```

Now we import the data as a dataframe with `read.csv` and look at all the fields/columns by printing out the `colnames.` Note that each row of the dataframe contains information about *one tweet*. We print out the first row with `head(tweets_df,1)` to get an idea of what kind of information is in each row and column.
```{r}
tweets_df <- read.csv("data/twitter_data.csv",
                      stringsAsFactors = FALSE,
                      encoding="UTF-8")
colnames(tweets_df)
head(tweets_df,1)
```
We describe a few of the notable columns in the following table as they relate to the first row of the data.

|Column|Description|Value|
|--|---|---|
|`text`|Raw tweet text|`r tweets_df$text[1]`|
|`uuidgens_user`|Unique ID of the user who tweeted (or retweeted) this tweet|`r tweets_df$uuidgens_user[1]`|
|`uuidgens_retweet`|Unique ID of the user that this tweet was *retweeted from*|`r tweets_df$uuidgens_retweet[1]`|
|`created_at`|Date and time tweet was tweeted|`r tweets_df$created_at[1]`|
|`display_text_width`|Number of characters in tweet|`r tweets_df$display_text_width[1]`|
|`location`|Location the tweeting user has listed in their Twitter bio|`r tweets_df$location[1]`|
|`description`|Description the tweeting user has listed in their Twitter bio|`r tweets_df$description[1]`|
|`long` and `lat`|Longitude and latitude coordinates of the user when tweet was created|`r tweets_df$long[1]`|

"NA" in the `uuidgens_retweet` row indicates the tweet was not a retweet, which we can also see in the column `is_retweet` where the value is "FALSE".

We point out that the columns beginning with "uuidgens" did not come with `rtweet` package. They were generated in a separate file to remove identifiers to avoid assuming or exploiting political or religious views and/or the state of someone's health. When importing the data with `rtweet`, there are many columns with identifiers. Each tweet has an associated user `screen_name` (usually referred to as the user's Twitter handle), a `name`, and `user_id.` Every Twitter user has a unique user ID, however these IDs are publicly traceable to the user names. Thus, all of these fields need to be removed, as well as related fields such as `retweet_user_id.`
Here is how we created the IDs:

- We first found all the unique user IDs from the columns `user_id`, `quoted_user_id`, and `retweet_user_id`
with `unique_ids <- unique(c(d$user_id, d$retweet_user_id, d$quoted_user_id))`.

- Then, we generated as many unique IDs as we had unique users with `dplR::uuid.gen` and created a dataframe with two columns: one with the original unique IDs, and the second with the generated unique IDs. 

- We merged this data three separate times with the tweets dataframe on the columns `user_id`, `quoted_user_id`, and `retweet_user_id` columns to create the columns `uuidgens_user`, `uuidgens_retweet`, and `uuidgens_quote.` 

To see the exact code, please refer to the appendix.

# Data wrangling
## Cleaning with regular expressions
Using regular expressions to parse and clean text is an important skill when dealing with text data. Let's take a look at the data to see what we need to clean by printing the head, or first 6 lines, of the `text` column - the raw text from the tweets we downloaded.

```{r}
head(unique(tweets_df$text),10)
```

From this small peak at the data, we can already see that there are a lot of strings that are not real words. We should remove text that does not help and, in fact, may hinder our analysis. For example, we observe lots of links ("https...") and UTF characters or emojis ("\\U000...") that aren't giving us any insightful information.

To do this, first we create a string named `pat` (short for *pattern*) that contains *regular expression* or "regex" patterns that match with various unwanted characters or phrases.

```{r}
pat <- "[\r\n]|&amp.|@.*?[ \t\r\n]|@.*?$|https:.*[ \t\r\n]|https:.*$|[^[:alpha:][ \t\r\n]?&/\\-#.']"
```

Note that the "|" character separates each pattern in the string. For example, the first pattern `[\r\n]` matches with newline characters, and is followed by a "|" to signify the end of that regular expression.

Let's break down each of the strings in `pat` and explain what text it is meant to match with.

|Regex Phrase|Text match|
|---|---|
|`[\r\n]`|Carriage returns and newline characters|
|`&amp.`|"&amp" phrase|The "." matches any character|
|`@.*?[ \t\r\n]`|Tagged user handle followed by white space|
|`@.*?$`|Tagged user handle at the end of the tweet|
|`https?:.*[ \t\r\n]`|Embedded link followed by white space|
|`https?:.*$`|Embedded link at the end of the tweet|
|`[^[:alnum:][:blank:]?&/\\-#.]`|Remove UTF-8 strings but keep any "?&/\\-#." characters|

Next, we break down one of the above patterns, `@.*?[ \t\r\n]`, to see how regular expressions work.

First, `@` simply matches with the @ sign, the beginning of tagging a user on Twitter. In regular expression "." can match with any character except for newline `\n`. The "`*`" character matches 0 or more of the previous character. For example, `s*` matches with "", "s", "ssss", etc. In this case we have `@.*`, so we are matching 0 or more of "." or any character. So now we have matched with anything beginning with @. Next, the "?" quantifier is *non-greedy*, meaning it matches as few characters as possible. We want to include this so that we remove the minimum amount of text and thus only the username tag. We stop removing text when we reach a space, tab, carriage return or newline character, `[ \t\r\n]`. Thus, `@.*?[ \t\r\n]` matches with any username that is retweeted or tagged. Let's see what happens when we use only this pattern in `str_replace_all`, which matches all occurrences of the pattern (hence, the addition of `_all`). Each match is replaced with an empty string `""` thus removing the unwanted string.

```{r}
remove_handle <- tweets_df %>% 
  mutate(text = str_replace_all(text, pattern = '@.*?[ \t\r\n]', "")) %>%
  mutate(retweet_text = str_replace_all(retweet_text, pattern = '@.*?[ \t\r\n]', ""))

head(remove_handle$text)
```
Great, it removed all the twitter handles found within the text!

Now we will remove the strings that match with the all the patterns in `pat` with `str_replace_all`. This time, it will check for matches with *every seperate pattern* in `pat` and again look for repeats of any pattern.
We replace the `tweets_df` columns `text` and `retweet_text` with the cleaned versions of the columns by using `mutate`. We also replace double spaces with single spaces in case the removal of some phrases left extra spaces behind. We print the head of the `text` column to view our changes.


```{r}
tweets_df <- tweets_df %>% 
  mutate(text = str_replace_all(text, pattern = pat, "")) %>%
  mutate(retweet_text = str_replace_all(retweet_text, pattern = pat, "")) %>%
  
  mutate(text = str_replace_all(text, pattern = "  ", " ")) %>%
  mutate(retweet_text = str_replace_all(retweet_text, pattern = "  ", " "))

head(unique(tweets_df$text))
```
Now that the raw tweet text is clean, let's start manipulating the tweets with natural language processing techniques to gain insight about our data.

## The tidy text and NLP

The tidy text format is using **one-token-per-row** in our dataframe. In short, tokens in natural language processing are words. To be technical, you could say a token is a list of characters between two spaces. Tokenization is the process of breaking up a string, paragraph, or document into tokens.

We want to tokenize our tweets, however we don't want to lose any information. Specifically, we want to keep track of what Tweet each word came from since our analysis involves differentiating tweets. Other analyses including topic summarization may not require keeping track of this--you can just a create a jumble (i.e. "corpus") of words and find commonalities.

First, we create the dataframe `text_df` with the `tweets_df` column `text` and add the `int` column as an index for the tweets. 
```{r}
unique_indices <- order(tweets_df$text)[!duplicated(sort(tweets_df$text))]

text_df <- data.frame("text" = tweets_df$text[unique_indices])
text_df$int <- as.numeric(unique_indices)
      
text_df$text <- as.character(text_df$text)


text_df <- text_df[order(text_df$int),]
row.names(text_df) <- 1:nrow(text_df)

head(text_df)
```
Next, we tokenize the text data with tidytext's `unnest_tokens` by using the `text` column as the input and `word` as the name of the output column. We replace `text_df` with this newly formatted dataframe.
```{r}
text_df <- text_df %>%
  unnest_tokens(word, text)

head(text_df,10)
```
Now that we have our data in a "tidy" format, we want to remove stop words from the text. *Stop words* in NLP are common words that don't add value to a sentence, paragraph, document etc. For example, "still think the flu shot is..." doesn't give us any extra information than "still think flu shot," so we can remove "the" and "is" to make our dataset more concise.

You can find lists of stop words in the `stop_words` dataset, which has two columns: `word` and `lexicon`. There are three different lexicons that reference the *source* of the stop word:

- [onix](http://www.lextek.com/manuals/onix/stopwords1.html)

- [SMART](http://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf)

- [snowball](http://snowball.tartarus.org/algorithms/english/stop.txt)

The difference between the lexicons is they include different words. Some include more than others, thus if we use these we remove more words and from our data. To quickly see these differences, we can count the number of stop words in each category using `dplyr::group_by`.
```{r}
data(stop_words)
head(stop_words)

stop_words %>% 
    group_by(lexicon) %>% 
    tally()
```
At this point in the analysis, we have no prior reason to chose a specific lexicon, so let's remove all of the stop words in the `stopwords` dataset from our data using the `anti_join` function. This will remove any word found in any of the three lexicons. We can always modify this later on.
```{r}
text_df <- text_df %>%
  anti_join(stop_words)

head(text_df,10)
```
If you compare this dataframe to `text_df` before the anti-join, you notice that the words "no", "to", "be", and "need" have been removed.

If we wanted to select a specific lexicon, such as the smallest set of stop words "snowball", we would use `dplyr::filter` command as shown below.
```{r eval = FALSE}
text_df <- text_df %>%
  anti_join(stop_words %>% filter(lexicon == "snowball"))
```

## Stemming
Another technique often used alongside removing stop words is word *stemming*. Stemming changes each word to a root word so that different conjugations of the same word are not treated as different words completely. A human can tell that "vaccine", vaccinate" and "vaccination" have close to the same meaning. However, for example, if we were to count every repetition of "vaccine" in R we would not get an accurate total since "vaccination" would be ignored. Instead, we apply a stemming algorithm which changes each of those words to "vaccin."

We will use the library `SnowballC` which implements the [Porter stemming algorithm](http://snowball.tartarus.org/algorithms/porter/stemmer.html). The command `SnowballC::wordStem` returns the stem of each word in a given vector.

```{r}
text_df_stemmed <- text_df %>%
  mutate(word = wordStem(word))
head(text_df_stemmed,10)
```
Since we've filtered and removed the most common English words--the stop words--let us also remove the most *uncommon* words in our dataset. This helps reduce the size of our data and remove meaningless words to this analysis, such as uncommon pronouns.

First, we create a temporary dataframe `temp` where each row contains a unique word from our data followed by the number of times this word is repeated in the dataset.
```{r}
temp <- text_df_stemmed %>%
  dplyr::count(word)
head(temp)
```
Next we create a dataframe with the most uncommon words. We can start with words repeated only once.
```{r}
uncomm_words <- temp %>% 
  filter(n == 1) %>% 
  select(word)
uncomm_words
```
Now that we have a dataframe of uncommon words, we use the `dplyr::anti_join` function again to remove these words from the dataframe.
```{r}
text_df_stemmed_common <- text_df_stemmed %>%
  anti_join(uncomm_words)

nrow(text_df_stemmed)
nrow(text_df_stemmed_common)
```
We have removed `r nrow(text_df_stemmed) - nrow(text_df_stemmed_common)` words.

# Exploratory data analysis

## Map visual
For a fun visual, we can visualize our tweets on a map. First let's find which tweets use the word or hashtag "antivax" and which tweets say "vaccines work." Next we add the column `Hashtags` that contains labels for if the tweet contains these words or contains neither.
```{r}
anti_lst <- as.numeric(
  grepl("antivax", tweets_df$text))
pro_lst <-as.numeric(
  grepl("vaccines work", tweets_df$text))

head(anti_lst)

tweets_df$Hashtags <- "none"
tweets_df$Hashtags[anti_lst == 1] <- "antivax"
tweets_df$Hashtags[pro_lst == 1] <- "vaccines work"
```
Next we use `ggplot2` and `ggmap` to plot this on a map.
```{r}
states <- map_data("state")

ggplot(data = states) + 
  geom_point(data = tweets_df, aes(x = lng, y = lat,  colour = Hashtags), size = 1) +
  geom_polygon(aes(x = long, y = lat, group=group), fill = NA, color = "dark grey") +
  theme_void()
```

As we can there is some missing latitude and longitude data. This is expected, since we assume most users do not have their location turned on on  twitter. We calculate exactly what percentage of tweets do not have an associated latitude and longitude below.
```{r}
(have_lat = sum(is.na(tweets_df$lat) ==FALSE)/dim(tweets_df)[1]*100)
```
We see that only `r have_lat` percent of tweets in this dataset have latitude and longitude coordinates.
```{r}
(have_hashtag = sum(tweets_df$Hashtags !="none")/dim(tweets_df)[1]*100)
```
Additionally, only `r have_hashtag` percent of tweets contain the phrases "antivax" or "vaccines work," so there is not much data on our map.

One solution to mitigate this lack-of-data issue is to use the `location` column. As described in the "Data import" section, this column contains the location a user lists in their Twitter bio. For example, users who live in Baltimore might write "Baltimore, MD." Others are more creative and don't put a real location, while others might write something more general such as "Maryland, USA." Let's print out some of the locations to observe.
```{r}
head(tweets_df$location,15)
```
We see that the format of most of the locations is a little different, however there are a few with the form, "City, State Abbrv." Using the package `localgeo`, let's convert one of the location entries to latitude and longitude coordinates.

The function `localgeo::geocode` takes two strings as input: a city, starting with a capital letter, and a state abbreviation in all capital letters.
`devtools::install_github("hrbrmstr/localgeo")`
```{r}
geocode("Baltimore","MD")
geocode("baltimore","MD")
geocode("Baltimore","Maryland")
geocode("Baltimore","md")
```
Now we can see that we must put our data in the exact format needed for the package. We select the fourth row value, "Bothell, WA" to use as a test. We first split the entry "Bothell, WA" into two strings using `strsplit`. Then we remove the unwanted extra space, " ", with `str_replace_all`.
```{r}
(loc <- unlist(strsplit(tweets_df$location[4], "[,]")))
(loc2 <- str_replace_all(loc, pattern = " ", ""))
geocode(loc2[1],loc2[2])
```
To make this apply more generally, let's remove all spaces after commas before we split the strings.
```{r}
no_space <- str_replace_all(tweets_df$location, pattern = ", ", ",")
test_loc <- strsplit(no_space, "[,]")
test_loc[1:4]
```
Next we use `sapply` to get all the element in the first column of each list. For the function in sapply, we select the whole column with the operator `'['`. Next, we specify which column we would like to select. In this case we are going use the first column as the city and the second column as the state abbreviation.
```{r}
cities <- sapply(test_loc,'[',1)
state_abbvs<- sapply(test_loc,'[',2)

lat_fromloc <- geocode(city=cities,state=state_abbvs)
head(lat_fromloc)
nrow(lat_fromloc)
```

Notice that there are `r nrow(lat_fromloc)` rows when we were expecting `r nrow(tweets_df)`. Why is this so? After much digging, we find a bug in the package: using `geocode` on the city *Muskegon, MI* returns two rows instead of one. So we add a temporary fix to remove exactly one extra row.
```{r}
geocode("Muskegon", "MI")
# temporary bug fix
if (nrow(lat_fromloc) == 5001){
  lat_fromloc <- lat_fromloc[-293,]
}
nrow(lat_fromloc)
```
Next, we add this data to the `lat` and `lng` columns with values `NA` and calculate how much of the total data now have latitude and longitude values.
```{r}
keep_lat <- is.na(tweets_df$lat) == FALSE
empty_lat <- is.na(tweets_df$lat)
tweets_df$lat[keep_lat][1:6]
lat_fromloc$lat[keep_lat][1:6]

tweets_df$lat[empty_lat] <- lat_fromloc$lat[empty_lat]
tweets_df$lng[empty_lat] <- lat_fromloc$lon[empty_lat]

(new_lat_percent = sum(is.na(tweets_df$lng) == FALSE)/dim(tweets_df)[1]*100)
```
Using the `location` column data, we easily converted `r new_lat_percent` percent of the data to latitude and longitude coordinates! Now we plot the map again to show how much more data we have now.
```{r}
ggplot(data = states) + 
  geom_point(data = tweets_df, aes(x = lng, y = lat,  colour = Hashtags), size = 1) +
  geom_polygon(aes(x = long, y = lat, group=group), fill = NA, color = "dark grey") +
  theme_void()
```

## N-grams
A common NLP data exploration practice is looking at n-gram frequencies.
*N-grams* are sequences of *n* items/tokens. Common n-grams that are used in NLP are unigrams, bigrams, and trigrams, meaning one word, two word, and three word combinations.

We find the top 10 most common words or unigrams to see common topics in our data. Recall that we have already removed stop words and unnecessary items such as web links, and we have stemmed the remaining words.

```{r}
unigram_count <- text_df_stemmed %>% dplyr::count(word, sort = TRUE)
head(unigram_count,20)
```
We can graph the unigram frequencies with `ggplot2` for a nice visualization of the most common words in our dataset. We use the `dplyr::top_n` function to select the 12 most frequent words, `reorder` to order them by most frequent to least, and `ggplot` to add plotting attributes.
```{r}
unigram_count %>%
  top_n(12) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  ylab(NULL) +
  coord_flip()
```

Next, we can look at common *bigrams*, which are two words in a row. The tidytext function `unnest_tokens` is an efficient way to find bigrams by setting `token = "ngrams"` and `n = 2`. The output column is named "bigram".

To get the most frequent bigrams we can simply use the `dplyr` library's `count` function again.
```{r}
bigrams_stemmed <- text_df_stemmed %>%
  unnest_tokens(bigram, word, token = "ngrams", n = 2)

head(bigrams_stemmed)
```

```{r}
head(bigrams_stemmed %>%
  dplyr::count(bigram, sort = TRUE),10)
```

Last in this section, a fun exploration graphic of unigrams is a "word cloud." We can create this data visualization easily with the `wordcloud2 `.
```{r}
unigram_count %>%
  with(wordcloud(word, n, max.words = 100))
```

## Sentiment
We want to classify the tweets by sentiment, or rather by the feelings and emotions portrayed by the user. As humans, we can usually tell if someone is angry, sad, satisfied, or even sarcastic. However we have to give the computer some clues on how to do this. We start with the `sentiments` dataset, which is loaded with the `tidytext` library. Similar to the `stop_words` dataset, there are three lexicons for each word:

* `afinn` gives each word an integer score between -5 and 5, from most negative to most positive sentiment.

* `bing` gives a binary "negative" or "positive" sentiment to each word.

* `nrc` assigns sentiment labels of "positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust."

```{r}
head(sentiments)
```
We can also use the tidytext's `get_sentiments()` function to only get data from a particular lexicon.
```{r}
head(get_sentiments('afinn'))
```
The words in the first 6 rows of the above dataset happen to have a score of -2, indicating mildly negative sentiment.

To learn about how the `sentiments` dataset was constructed, read the following quote from [Chapter 2](https://www.tidytextmining.com/sentiment.html) of the online textbook, Text Mining with R:

*"How were these sentiment lexicons put together and validated? They were constructed via either crowdsourcing (using, for example, Amazon Mechanical Turk) or by the labor of one of the authors, and were validated using some combination of crowdsourcing again, restaurant or movie reviews, or Twitter data. Given this information, we may hesitate to apply these sentiment lexicons to styles of text dramatically different from what they were validated on, such as narrative fiction from 200 years ago. While it is true that using these sentiment lexicons with, for example, Jane Austenâ€™s novels may give us less accurate results than with tweets sent by a contemporary writer, we still can measure the sentiment content for words that are shared across the lexicon and the text."*


Next let's merge our Twitter data with the sentiments data to explore the sentiment of the language used in the tweets. For a quick example, we will use the "afinn" lexicon and set the score to -2 to find some mildly negative words in the Twitter data.

First we use the `get_sentiments` function to get only the "afinn" lexicon data. We then filter the data so that we only have words with scores of -2.
```{r}
nrc_example <- get_sentiments("afinn") %>% 
  filter(score == -2)
```
Next, we use the `inner_join` function to find words that are in both our Twitter dataset and in our sample sentiments dataset: words that have a sentiment score of -2. We sort them by the most frequent and print the first 10 rows with `head`.
```{r}
head(text_df %>%
  inner_join(nrc_example) %>%
  dplyr::count(word, sort = TRUE),10)
```

Since the "afinn" lexicon is numerical, we can plot the average "afinn" scores for each tweet to explore the overall sentiment of our data.

```{r,message=FALSE}
library(tidyr)
head(text_df)

tweet_sentiment <- text_df %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(int) %>%
  mutate(text = paste0(word)) %>%
  summarize(sentiment_mean = mean(score))

tweet_sentiment_ordered <- tweet_sentiment[order(tweet_sentiment$sentiment_mean),] 

tweet_sentiment_ordered$int <- factor(tweet_sentiment_ordered$int, levels = tweet_sentiment_ordered$int)

ggplot(tweet_sentiment_ordered, aes(sentiment_mean)) +
  geom_histogram(show.legend = FALSE, binwidth = .3) +
  ggtitle("Mean afinn sentiment score per tweet") + 
  geom_vline(xintercept = 0, linetype = "dashed", colour = "red") +
  ylab(NULL)
```
Let's print out some positive tweets. We can use the index column "int" that we've kept constant to print out the original (sans and Twitter handles) tweets.
```{r}
positive <- tweet_sentiment$int[tweet_sentiment$sentiment_mean > 2]
head(tweets_df$text[positive])
```
And let's look at some negative ones as well..
```{r}
negative <- tweet_sentiment$int[tweet_sentiment$sentiment_mean < -2 ]
head(tweets_df$text[negative])
```
Neither the positive nor negative tweets are unanimously pro- or anti-vax, even with this small sample of text. Because of this, we will not be able to use sentiment as a method to classify tweets, but rather as an evaluation metric.

# Data analysis

## Topic Modeling
Since our data is unsupervised, meaning we don't have any prior knowledge or labels that tell us if a tweet is pro- or anti-vaccination, we are going to use *topic modeling* to separate the tweets into the two sides using only the text. 

Latent Dirichlet allocation (LDA) is common topic modeling method: it views each document (or in our case, each tweet) as a mixture of topics, and each topic as a combination of words. A word can be part of multiple topics, thus allowing some overlap.

In order to apply LDA, we need to take our cleaned and "tidy" text data `text_df` and turn it into a *document term matrix* (DTM). It is easy to convert from the tidy text format to this matrix. First, we add a column counting the number of word repeats per tweet.
```{r}
dtm_df <- text_df_stemmed_common %>%
  group_by(int, word) %>%
  tally()

head(dtm_df)
```
Next, we simply use the command `tidytext::cast_dtm` to convert to a document term matrix, a structure where each row is a document and each column is a word. You can explore the DTM in your console with the command `text_dtm$` followed by "i", "j", "v", "nrow", "ncol", or "dimnames."
```{r}
(text_dtm <- dtm_df %>%
  cast_dtm(int, word, n))

typeof(text_dtm)

head(Terms(text_dtm))
```
We see that the terms in the DTM are the words from each tweet.
```{r}
head(Docs(text_dtm))
```
The "Docs" or documents are simply the tweets, numbers 1 through `{r} text_dtm$nrow`.

Now we are ready to apply LDA to our DTM. We set `k=2` because we are trying to create two categories of tweets: pro and anti vaccination. However, we should consider trying 3 categories later, the third being "other" tweets that are irrelevant to the debate. Note that we have no control over whether these will be the two or three clear categories, but it is the goal we are trying to achieve.

We also set an arbitrary seed to ensure we recreate the same results each time we run the code.
```{r}
(lda_k2 <- LDA(text_dtm, k = 2))#, #control = list(seed = 1234)))
```
Now that we have our model results, we must do some exploring in order to interpret them.

As mentioned previously, each word can be in multiple topics. To examine the probability that a certain word belongs to one of the topics, we can retrieve the "beta" values. This can be done easily by providing the function `tidy` our topic model.
```{r}
topics_k2 <- tidy(lda_k2, matrix = "beta")
head(topics_k2)
```
Notice this dataframe is again in the "tidy" format with one word/topic per row.

We can plot the most probable words in each topic by first creating a dataframe with the top 15 words for each topic.

Then we sort by the top terms and plot the data with `ggplot2`. In this case we filter by topic to show the differences.
```{r}
top_terms <- topics_k2 %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = TRUE) +
  coord_flip()
```

Some words overlap in each topic, but this is okay (and even good) because we know words such as "vaccine" can be used by both sides of this debate.

To look at the most polarizing words, i.e. words with the greatest difference between the beta values for topic 1 and 2 we can take the log ratio. We filter the `topics_k2` tidy dataframe to only have common words by setting `topic_x > .001`.
```{r}
beta_spread <- topics_k2 %>%
  mutate(topic = paste0("topic_", topic)) %>%
  spread(topic, beta) %>%
  filter(topic_1 > .001 | topic_2 > .001) %>%
  mutate(log_ratio = log2(topic_1 / topic_2))

head(beta_spread,10)
nrow(beta_spread)
```
Next we filter the log ratio to display just a few common terms with the largest differences between beta values. 
Note that since our log ratio is $log(\frac{\beta_{topic_1}}{\beta_{topic_2}})$, if the beta value for topic 1 is larger, the log ratio will be positive. Again this means that the particular word is more likely to fall in topic 1 instead of 2. The reverse is true (if $\beta_{t2} > \beta_{t1}$, then the log is negative).
```{r}
beta_spread %>% 
  filter(log_ratio > 4.5 | log_ratio< -4.5) %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(term, log_ratio)) +
  geom_col(show.legend = FALSE) +
  coord_flip()
```
The graph above displays words with the largest magnitude log ratios, out of the `r nrow(beta_spread)` most frequent words in either topic. The positive words are more likely to be in topic 1, and the negative words are more likely to be in topic 2.

### Examine group probabilities by tweet

Now that we have examined the *beta* values, or probability a word is in each topic, we now look at the *gamma* $(\gamma)$ probabilities or "per-document-per-topic probabilities."

We create a new dataframe using `tidy()` and observe how it is arranged: for each topic and document combination (for this dataset, each document is a tweet), there is a gamma value between 0 and 1. 
```{r}
lda_tweets <- tidy(lda_k2, matrix = "gamma")
head(lda_tweets)
```
Next, we look at the tweets with the most extreme $\gamma$ values to get a sense of what we are working with.
```{r}
lda_tweets<-lda_tweets %>% arrange(desc(gamma))
head(lda_tweets)
```
The way to interpret the above information is as follows: tweet `r lda_tweets$document[1]` is more likely to belong in topic `r lda_tweets$topic[2]` because it has a gamma value of `r lda_tweets$gamma[1]` for that topic. Tweet `r lda_tweets$document[2]` is more likely to belong in topic `r lda_tweets$topic[2]` because it has a gamma value of `r lda_tweets$gamma[2]` for that topic, etc.

This doesn't tell us that much until we actually read the tweets in relation to their assigned topics. 

Let's print the 20 tweets with the largest $\gamma$ scores,meaning they are the most skewed towards one topic or the other. We do this using the `kableExtra` library to make the twitter text nice and readable. We can also add a `scroll_box` to make a window that can be scrolled through when this file is knit to a HTML file.
```{r}
diff_tweets<-tweets_df$text[as.numeric(lda_tweets$document[1:20])]

diff_tweets<-data.frame(diff_tweets)
diff_tweets$topic <- as.numeric(lda_tweets$topic[1:20])

kable(diff_tweets) %>%
  kable_styling("striped", full_width = F) %>%
  scroll_box(width = "750px", height = "300px")
```
It's not obvious that there is a distinct difference between the two topic categories. In fact, there are some tweets that are clearly pro-vaccination in topic 2 but I also see some that are clearly anti-vaccination. This may be because this kind of topic modeling only looks at individual words such as "vaccine," and not at bigrams or trigrams such as "vaccines work" and "vaccines cause autism."

So let's run a similar analysis but instead of starting with the one-token-per-row format, we start with the `bigrams` dataframe we created earlier. Take a look at the dataframe we just used for our analysis compared to `bigrams`:
```{r}
head(text_df_stemmed_common)
head(bigrams_stemmed)
```
The format is quite similar. Let's proceed in converting `bigrams` to a document term matrix (DTM).
```{r}
dtm_bigram_df <- bigrams_stemmed %>%
  group_by(int, bigram) %>%
  tally()

head(dtm_bigram_df)

(bigram_dtm <- dtm_bigram_df %>%
  cast_dtm(int, bigram, n))

head(terms <- Terms(bigram_dtm))
```

Now we apply LDA to this DTM as before.

This time, let's try using `k = 3` to see if we can capture the following three topics:

- Anti-Vaccination
- Pro-Vaccination
- Irrelevant/No Opinion


```{r}
(lda_bi_k3 <- LDA(bigram_dtm, k = 3, control = list(seed = 1234)))
```
We get the beta values as before.
```{r}
bigram_beta <- tidy(lda_bi_k3, matrix = "beta")
head(bigram_beta)
```

```{r}
bigram_gamma <- tidy(lda_bi_k3, matrix = "gamma")
bigram_gamma<-bigram_gamma %>% arrange(desc(gamma))
head(bigram_gamma)
```
Wow! These $\gamma$ values are much higher than when we used unigrams and 2 topics. Let's read the tweets to see how it did.
```{r}
diff_tweets<-tweets_df$text[as.numeric(bigram_gamma$document[1:20])]

diff_tweets<-data.frame(diff_tweets)
diff_tweets$topic <- as.numeric(bigram_gamma$topic[1:20])

kable(diff_tweets) %>%
  kable_styling("striped", full_width = F) %>%
  scroll_box(width = "750px", height = "300px")
```
Topic 2 has many tweets that seem pro-vaccination. However the other two topics are difficult to distinguish.

# Summary of results
This analysis has demonstrated the difficulty of characterizing unsupervised text data. While we can create groups through topic summarization, we cannot quantify how accurate our algorithm works because we do not have any truth data.

One solution to having unlabeled data of this structure is to use a *semi-supervised* machine learning framework and completing a social network analysis. We can select *seed users*, or prominent figures that clearly have one opinion or another to create labels for part of the data. For example, the World Health Organization and CDC are two entities that would produce pro-vaccination tweets. Using social network analysis would allow us to use algorithms such as the Label Propagation Algorithm to identify users with the same opinion as these seed users. Structuring the network can be done in many different ways: we can connect users who retweeted one another or users that retweet the same tweet. We can also connect tweets by sentiment and key words.

In conclusion, Twitter data is fascinating to analyze: although it is messy and sparse, there is much insight to be gleaned from studying this data. Moreover, analysis can be completed in real time since there is a constant stream of tweets.